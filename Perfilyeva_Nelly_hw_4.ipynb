{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Алгоритмы интеллектуальной обработки больших объемов данных\n",
    "## Домашнее задание №4 - Градиентный бустинг\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Общая информация**\n",
    "\n",
    "**Срок сдачи:** 10 мая 2021, 08:30   \n",
    "**Штраф за опоздание:** -2 балла после 08:30 10 мая, -4 балла после 08:30 17 мая, -6 баллов после 08:30 24 мая, -8 баллов после 08:30 31 мая.\n",
    "\n",
    "При отправлении ДЗ указывайте фамилию в названии файла Присылать ДЗ необходимо в виде ссылки на свой github репозиторий на почту ml1.sphere@mail.ru с указанием темы в следующем формате:\n",
    "[ML0221, Задание 4] Фамилия Имя. \n",
    "\n",
    "\n",
    "Используйте данный Ipython Notebook при оформлении домашнего задания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Считаем производные для функций потерь (1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем реализовать градиентный бустинг для 3 функций потерь:\n",
    "\n",
    "1) MSE  $L(a(x_i), y_i) = (y_i - a(x_i)) ^ 2$\n",
    "\n",
    "2) Экспоненциальная  $L(a(x_i), y_i) = exp( -a(x_i) y_i), y_i \\in \\{-1, 1\\}$\n",
    "\n",
    "3) Логистическая  $L(a(x_i), y_i) = \\log (1 + exp( -a(x_i) y_i)), y_i \\in \\{-1, 1\\}$\n",
    "\n",
    "где $a(x_i)$ предсказание бустинга на итом объекте. \n",
    "\n",
    "Для каждой функции потерь напишите таргет, на который будет настраиваться каждое дерево в бустинге. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ваше решение тут\n",
    "1) $$-\\frac{dL}{da} = 2(y_i - a(x_i))$$\n",
    "2) $$-\\frac{dL}{da} = y_i \\exp(-a(x_i)y_i)$$\n",
    "3) $$-\\frac{dL}{da} = \\frac{y_i \\exp(-a(x_i)y_i)}{1+ \\exp(-a(x_i)y_i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Реализуем градиентный бустинг (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте класс градиентного бустинга для классификации. Ваша реализация бустинга должна работать по точности не более чем на 5 процентов хуже чем GradientBoostingClassifier из sklearn. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Детали реализации:\n",
    "\n",
    "-- должно поддерживаться 3 функции потерь\n",
    "\n",
    "-- сами базовые алгоритмы(деревья, линейные модели и тп) реализовать не надо, просто возьмите готовые из sklearn\n",
    "\n",
    "-- в качестве функции потерь для построения одного дерева используйте MSE\n",
    "\n",
    "-- шаг в бустинге можно не подбирать, можно брать константный\n",
    "\n",
    "-- можно брать разные модели в качестве инициализации бустинга\n",
    "\n",
    "-- должны поддерживаться следующие параметры:\n",
    "\n",
    "а) число итераций \n",
    "\n",
    "б) размер шага\n",
    "\n",
    "в) процент случайных фичей при построении одного дерева\n",
    "\n",
    "д) процент случайных объектов при построении одного дерева\n",
    "\n",
    "е) параметры базового алгоритма (передавайте через **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGradientBoostingClassifier:\n",
    "\n",
    "    def __init__(self, loss='MSE', learning_rate=0.1, n_estimators=10,\n",
    "                 colsample=1, subsample=1, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        loss -- один из 3 лоссов\n",
    "        learning_rate -- шаг бустинга\n",
    "        n_estimators -- число итераций\n",
    "        colsample -- процент рандомных признаков при обучнеии одного алгоритма\n",
    "        subsample -- процент рандомных объектов при обучнеии одного алгоритма\n",
    "        args, kwargs -- параметры  базовых моделей\n",
    "        \"\"\"\n",
    "        self.loss = loss\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_estimators = n_estimators\n",
    "        self.colsample = colsample\n",
    "        self.subsample = subsample\n",
    "        self.args = args\n",
    "        self.kwargs = kwargs\n",
    "        self.__grad_func = self.__grad_exp\n",
    "        \n",
    "#         print(loss)\n",
    "        \n",
    "        if self.loss == 'MSE':\n",
    "            self.__grad_func = self.__grad_mse\n",
    "#             print('dfgdfg')\n",
    "        if self.loss == 'exp':\n",
    "            self.__grad_func = self.__grad_exp\n",
    "        if self.loss == 'logloss':\n",
    "            self.__grad_func = self.__grad_logloss \n",
    "#         print(self.__grad_func)\n",
    "    \n",
    "    def __grad_mse(self, y, a):\n",
    "#         print(2 * (y - a))\n",
    "        return 2 * (y - a)\n",
    "    \n",
    "    def __grad_exp(self, y, a):\n",
    "        return y * np.exp(-a * y)\n",
    "    \n",
    "    def __grad_logloss(self, y, a):\n",
    "        return y * np.exp(-a * y) / (1 + np.exp(-a * y))\n",
    "    \n",
    "    def get_indexes(self, X):\n",
    "        X_indexes = np.random.choice(X.shape[0], \n",
    "                                     int(self.X.shape[0] * self.subsample))\n",
    "        y_indexes = np.random.choice(X.shape[1], \n",
    "                                     int(self.X.shape[1] * self.colsample))\n",
    "        return X_indexes, y_indexes\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, base_model=DecisionTreeRegressor, init_model=None):\n",
    "        \"\"\"\n",
    "        X -- объекты для обучения:\n",
    "        y -- таргеты для обучения\n",
    "        base_model -- класс базовых моделей, например sklearn.tree.DecisionTreeRegressor\n",
    "        init_model -- класс для первой модели, если None то берем константу (только для посл задания)\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.__base_model = base_model\n",
    "        self.__init_model = init_model\n",
    "        self.classes = np.unique(y)\n",
    "        self.n_class = len(self.classes)\n",
    "        self.models = []\n",
    "        self.features = []\n",
    "        \n",
    "        if init_model is None:\n",
    "            self.init_model = None\n",
    "            self.target_predict=np.float64(np.ones(self.y.shape[0]) / self.y.shape[0])\n",
    "        else:\n",
    "            self.init_model = init_model(*self.args, **self.kwargs)\n",
    "            self.init_model.fit(X, y)\n",
    "            self.target_predict = np.float64(self.init_model.predict(X))\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            X_indexes, y_indexes = self.get_indexes(X)\n",
    "            X_train = X[X_indexes, :][:, y_indexes]\n",
    "            y_train = y[X_indexes]\n",
    "            X_test = X[:, y_indexes]\n",
    "            previous_pred = self.target_predict[X_indexes]\n",
    "            \n",
    "            self.features.append(y_indexes)\n",
    "            model = self.__base_model(*self.args, **self.kwargs)\n",
    "            self.models.append(model)\n",
    "            target = self.__grad_func(y_train, previous_pred)\n",
    "            model.fit(X_train, target)\n",
    "            self.target_predict += self.learning_rate * model.predict(X_test)\n",
    "        \n",
    "    def predict(self, X):\n",
    "\n",
    "        if self.init_model is None:\n",
    "            y_pred = np.float64(np.ones(X.shape[0]) / X.shape[0])\n",
    "        else:\n",
    "            y_pred = np.float64(self.init_model.predict(X))\n",
    "        for model, feat in zip(self.models, self.features):\n",
    "            y_pred += self.learning_rate * model.predict(X[:, feat])\n",
    "        return np.around(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_clf = MyGradientBoostingClassifier()\n",
    "clf = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = load_wine()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    wine.data, wine.target, test_size=0.1, stratify=wine.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "my_clf.fit(X_train, y_train)\n",
    "clf.fit(X_train, y_train)\n",
    "print(accuracy_score(y_pred=clf.predict(X_test), y_true=y_test))\n",
    "print(accuracy_score(y_pred=my_clf.predict(X_test), y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подбираем параметры (2 балла)\n",
    "\n",
    "Давайте попробуем применить Ваш бустинг для предсказаний цены домов в Калифорнии. Чтобы можно было попробовтаь разные функции потерь, переведем по порогу таргет в 2 класса: дорогие и дешевые дома."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В задании нужно\n",
    "\n",
    "1) Построить график точности в зависимости от числа итераций на валидации.\n",
    "\n",
    "2) Подобрать оптимальные параметры Вашего бустинга на валидации. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "X, y = fetch_california_housing(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 8) (20640,)\n"
     ]
    }
   ],
   "source": [
    "# Превращаем регрессию в классификацию\n",
    "y = (y > 2.0).astype(int)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "21\n",
      "41\n",
      "61\n",
      "81\n",
      "101\n",
      "121\n",
      "141\n",
      "161\n",
      "181\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "n_estimat = [x for x in range(1, 200, 20)]\n",
    "acc_score = []\n",
    "for n in n_estimat:\n",
    "    scores = []\n",
    "    print(n)\n",
    "    for train_ind, test_ind in kf.split(X):\n",
    "        X_train = X[train_ind]\n",
    "        y_train = y[train_ind]\n",
    "        X_test = X[test_ind]\n",
    "        y_test = y[test_ind]\n",
    "        my_clf = MyGradientBoostingClassifier(n_estimators=n)\n",
    "        my_clf.fit(X_train, y_train)\n",
    "        y_pred = my_clf.predict(X_test)\n",
    "        scores.append(accuracy_score(y_test, y_pred))\n",
    "    acc_score.append(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5780523255813954, 0.8160368217054262, 0.8000484496124031, 0.8035368217054263, 0.8040697674418604, 0.7947189922480621, 0.8040213178294573, 0.8022286821705427, 0.7974806201550387, 0.7984496124031008]\n"
     ]
    }
   ],
   "source": [
    "print(acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_score = [0.5780523255813954, 0.8160368217054262, 0.8000484496124031, 0.8035368217054263, 0.8040697674418604, 0.7947189922480621, 0.8040213178294573, 0.8022286821705427, 0.7974806201550387, 0.7984496124031008]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8160368217054262"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'accuracy')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(n_estimat, acc_score, 'r-.')\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'loss' : ['MSE', 'epx', 'logloss'], \n",
    "         'n_estimators' : [5, 10, 15, 20, 25], \n",
    "         'learning_rate' : [0.1, 0.2, 0.3, 0.4, 0.5], \n",
    "          'colsample' : [0.2, 0.5, 0.7, 1],\n",
    "          'subsample' : [0.2, 0.5, 0.7, 1], \n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE\n",
      "epx\n",
      "logloss\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "loss = ['MSE', 'epx', 'logloss']\n",
    "acc_score = []\n",
    "for l in loss:\n",
    "    scores = []\n",
    "    for train_ind, test_ind in kf.split(X):\n",
    "        X_train = X[train_ind]\n",
    "        y_train = y[train_ind]\n",
    "        X_test = X[test_ind]\n",
    "        y_test = y[test_ind]\n",
    "        my_clf = MyGradientBoostingClassifier(loss=l)\n",
    "        my_clf.fit(X_train, y_train)\n",
    "        y_pred = my_clf.predict(X_test)\n",
    "        scores.append(accuracy_score(y_test, y_pred))\n",
    "    acc_score.append(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7996608527131783, 0.7762596899224807, 0.5780523255813954]\n",
      "0.7996608527131783\n",
      "MSE\n"
     ]
    }
   ],
   "source": [
    "print(acc_score)\n",
    "print(np.max(acc_score))\n",
    "print(loss[np.argmax(acc_score)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['loss'] = loss[np.argmax(acc_score)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss = MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 'MSE', 'n_estimators': 25}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "n_estimators = [5, 10, 15, 20, 25]\n",
    "acc_score = []\n",
    "for n in n_estimators:\n",
    "    scores = []\n",
    "    for train_ind, test_ind in kf.split(X):\n",
    "        X_train = X[train_ind]\n",
    "        y_train = y[train_ind]\n",
    "        X_test = X[test_ind]\n",
    "        y_test = y[test_ind]\n",
    "        my_clf = MyGradientBoostingClassifier(n_estimators = n, **params)\n",
    "        my_clf.fit(X_train, y_train)\n",
    "        y_pred = my_clf.predict(X_test)\n",
    "        scores.append(accuracy_score(y_test, y_pred))\n",
    "    acc_score.append(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.764437984496124, 0.7838662790697675, 0.8103682170542635, 0.8000484496124031, 0.8105620155038759]\n",
      "0.8105620155038759\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "print(acc_score)\n",
    "print(np.max(acc_score))\n",
    "print(n_estimators[np.argmax(acc_score)])\n",
    "params['n_estimators'] = n_estimators[np.argmax(acc_score)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_estimators = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "learning_rate = [0.1, 0.2, 0.3, 0.4, 0.5] \n",
    "acc_score = []\n",
    "for l in learning_rate:\n",
    "    scores = []\n",
    "    for train_ind, test_ind in kf.split(X):\n",
    "        X_train = X[train_ind]\n",
    "        y_train = y[train_ind]\n",
    "        X_test = X[test_ind]\n",
    "        y_test = y[test_ind]\n",
    "        my_clf = MyGradientBoostingClassifier(learning_rate = l, **params)\n",
    "        my_clf.fit(X_train, y_train)\n",
    "        y_pred = my_clf.predict(X_test)\n",
    "        scores.append(accuracy_score(y_test, y_pred))\n",
    "    acc_score.append(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8091569767441861, 0.7774224806201551, 0.7246124031007751, 0.6005813953488371, 0.5354651162790697]\n",
      "0.8091569767441861\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "print(acc_score)\n",
    "print(np.max(acc_score))\n",
    "print(learning_rate[np.argmax(acc_score)])\n",
    "params['learning_rate'] = learning_rate[np.argmax(acc_score)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "colsample = [0.2, 0.5, 0.7, 1]\n",
    "acc_score = []\n",
    "for c in colsample:\n",
    "    scores = []\n",
    "    for train_ind, test_ind in kf.split(X):\n",
    "        X_train = X[train_ind]\n",
    "        y_train = y[train_ind]\n",
    "        X_test = X[test_ind]\n",
    "        y_test = y[test_ind]\n",
    "        my_clf = MyGradientBoostingClassifier(colsample = c, **params)\n",
    "        my_clf.fit(X_train, y_train)\n",
    "        y_pred = my_clf.predict(X_test)\n",
    "        scores.append(accuracy_score(y_test, y_pred))\n",
    "    acc_score.append(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6354651162790697, 0.7456879844961241, 0.7891472868217054, 0.787015503875969]\n",
      "0.7891472868217054\n",
      "0.7\n"
     ]
    }
   ],
   "source": [
    "print(acc_score)\n",
    "print(np.max(acc_score))\n",
    "print(colsample[np.argmax(acc_score)])\n",
    "params['colsample'] = colsample[np.argmax(acc_score)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "subsample = [0.2, 0.5, 0.7, 1] \n",
    "acc_score = []\n",
    "for s in subsample:\n",
    "    scores = []\n",
    "    for train_ind, test_ind in kf.split(X):\n",
    "        X_train = X[train_ind]\n",
    "        y_train = y[train_ind]\n",
    "        X_test = X[test_ind]\n",
    "        y_test = y[test_ind]\n",
    "        my_clf = MyGradientBoostingClassifier(subsample = s, **params)\n",
    "        my_clf.fit(X_train, y_train)\n",
    "        y_pred = my_clf.predict(X_test)\n",
    "        scores.append(accuracy_score(y_test, y_pred))\n",
    "    acc_score.append(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7250968992248061, 0.7847383720930232, 0.7751453488372093, 0.789001937984496]\n",
      "0.789001937984496\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(acc_score)\n",
    "print(np.max(acc_score))\n",
    "print(subsample[np.argmax(acc_score)])\n",
    "params['subsample'] = subsample[np.argmax(acc_score)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 'MSE',\n",
       " 'n_estimators': 25,\n",
       " 'learning_rate': 0.1,\n",
       " 'colsample': 0.7,\n",
       " 'subsample': 1}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True, random_state = 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8771802325581395"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_clf = MyGradientBoostingClassifier(**params)\n",
    "my_clf.fit(X_train, y_train)\n",
    "accuracy_score(y_test, my_clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BooBag BagBoo (1 балл)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем объединить бустинг и бэгинг. Давайте\n",
    "\n",
    "1) в качестве базовой модели брать не дерево решений, а случайный лес (из sklearn)\n",
    "\n",
    "2) обучать N бустингов на бустрапированной выборке, а затем предикт усреднять"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуйте обе этих стратегии на данных из прошлого задания. Получилось ли улучшить качество? Почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8839631782945736"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_clf = MyGradientBoostingClassifier(**params)\n",
    "my_clf.fit(X_train, y_train, base_model=RandomForestRegressor)\n",
    "accuracy_score(y_test, my_clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(X, y):\n",
    "    n_samples = len(X)\n",
    "    indx = np.random.randint(0, len(X), len(X))\n",
    "    return X[indx], y[indx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "for N in range(15):\n",
    "    X_bootstrap, y_bootstrap = get_samples(X_train, y_train)\n",
    "    my_clf = MyGradientBoostingClassifier(**params)\n",
    "    my_clf.fit(X_bootstrap, y_bootstrap)\n",
    "    clf.append(my_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = [my_clf.predict(X_test) for my_clf in clf]\n",
    "y_pred = (np.mean(pred, axis=0) > 0.5).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8946220930232558"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество улучшилось. \n",
    "В первом случае предсказание строится путем усреднения предсказаний каждого из построенных деревьев, в результате получаем более несмещенную модель. \n",
    "В случае BagBoo за счет бустинга мы уменьшаем дисперсию построенных моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Умная инициализация (1 балл)\n",
    "\n",
    "Попробуйте брать в качестве инициализации бустинга не константу, а какой-то алгоритм и уже от его предикта стартовать итерации бустинга. Попробуйте разные модели из sklearn: линейные модели, рандом форест, svm..\n",
    "\n",
    "Получилось ли улучшить качество? Почему?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8607073643410853"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_clf = MyGradientBoostingClassifier()\n",
    "my_clf.fit(X_train, y_train)\n",
    "y_pred = my_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8616763565891473"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_clf = MyGradientBoostingClassifier()\n",
    "my_clf.fit(X_train, y_train, init_model=SVC)\n",
    "y_pred = my_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8347868217054264"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_clf = MyGradientBoostingClassifier()\n",
    "my_clf.fit(X_train, y_train, init_model=SGDClassifier)\n",
    "y_pred = my_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8832364341085271"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_clf = MyGradientBoostingClassifier()\n",
    "my_clf.fit(X_train, y_train, init_model=RandomForestRegressor)\n",
    "y_pred = my_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8829941860465116"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_clf = MyGradientBoostingClassifier()\n",
    "my_clf.fit(X_train, y_train, init_model=RandomForestClassifier)\n",
    "y_pred = my_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае инициализации RandomForest качество стало лучше, тк после инициализации RF получаем более точное решение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Фидбек (бесценно)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Какие аспекты обучения  ансамблей Вам показались непонятными? Какое место стоит дополнительно объяснить?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ваш ответ здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Здесь Вы можете оставить отзыв о этой домашней работе или о всем курсе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ВАШ ОТЗЫВ ЗДЕСЬ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
